{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a17d64",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a0edf",
   "metadata": {},
   "source": [
    "#### Libraries need to be installed\n",
    "1) pip install nltk\n",
    "2) pip install banglanltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b083403",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49d73427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: banglanltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install banglanltk\n",
    "\n",
    "\n",
    "import nltk\n",
    "import banglanltk as bn\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eab400",
   "metadata": {},
   "source": [
    "##### Use this data for further processing\n",
    "গতকাল ডিএসইতে ১ হাজার ১৯০ কোটি ২৬ লাখ টাকার শেয়ার লেনদেন হয়েছে। যা আগের দিন থেকে ৫ কোটি ১৫ লাখ! টাকা কম! গতকাল ডি.এসইতে ১ হাজার ১৯৫ কোটি ৪১ লাখ টাকার শেয়ার লেনদেন হয়েছিল। এর আগের দিন মঙ্গলবার ১ হাজার ১৮৩ কোটি টাকা শেয়ার লেনদেন হয়েছিল। বাজার বিশ্লেষণে দেখা যায় ডিএসই প্রধান সূচক ডিএসইএক্স ১২ পয়েন্ট বেড়ে অবস্হান করছে ৬ হাজার ৩১২ পয়েন্টে। অন্য সূচকগুলোর মধ্যে ডিএসইএস বা শরিয়াহ সূচক ৪ পয়েন্ট বেড়ে অবস্হান করছে ১ হাজার ৩৭৫ পয়েন্টে। এছাড়া ডিএস৩০ সূচক ৬ পয়েন্ট বেড়ে দাঁড়িয়েছে ২ হাজার ২৬৫ পয়েন্টে। দেশের প্রধান এই শেয়ারবাজারে গতকাল ৩৮১টি কোম্পানি ও মিউচুয়াল ফান্ডের শেয়ার লেনদেন হয়েছে। এর মধ্যে দর বেড়েছে ১২৯টির। কমেছে ১৯৯টির এবং অপরিবর্তিত রয়েছে ৫৩টির। অপর বাজার চট্টগ্রাম স্টক  এক্সচেঞ্জে সিএসই সিএসই সার্বিক সূচক সিএসপিআই ৭৮ point বেড়েছে। লেনদেন হয়েছে 21 core টাকার শেয়ার।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91510354",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"গতকাল ডিএসইতে ১ হাজার ১৯০ কোটি ২৬ লাখ টাকার শেয়ার লেনদেন হয়েছে। যা আগের দিন থেকে ৫ কোটি ১৫ লাখ টাকা কম! গতকাল ডি.এসইতে ১ হাজার ১৯৫ কোটি ৪১ লাখ টাকার শেয়ার লেনদেন হয়েছিল। এর আগের দিন মঙ্গলবার ১ হাজার ১৮৩ কোটি টাকা শেয়ার লেনদেন হয়েছিল। বাজার বিশ্লেষণে দেখা যায় ডিএসই প্রধান সূচক ডিএসইএক্স ১২ পয়েন্ট বেড়ে অবস্হান করছে ৬ হাজার ৩১২ পয়েন্টে। অন্য সূচকগুলোর মধ্যে ডিএসইএস বা শরিয়াহ সূচক ৪ পয়েন্ট বেড়ে অবস্হান করছে ১ হাজার ৩৭৫ পয়েন্টে। এছাড়া ডিএস৩০ সূচক ৬ পয়েন্ট বেড়ে দাঁড়িয়েছে ২ হাজার ২৬৫ পয়েন্টে। দেশের প্রধান এই শেয়ারবাজারে গতকাল ৩৮১টি কোম্পানি ও মিউচুয়াল ফান্ডের শেয়ার লেনদেন হয়েছে। এর মধ্যে দর বেড়েছে ১২৯টির। কমেছে ১৯৯টির এবং অপরিবর্তিত রয়েছে ৫৩টির। অপর বাজার চট্টগ্রাম স্টক এক্সচেঞ্জে সিএসই সিএসই সার্বিক সূচক সিএসপিআই ৭৮ point বেড়েছে। লেনদেন হয়েছে 21 core টাকার শেয়ার।\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2a1296e",
   "metadata": {},
   "source": [
    "# Tokenizing the words and provide the length  (use banglanltk library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "407aa5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bab9782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Tokenized Words: 133\n"
     ]
    }
   ],
   "source": [
    "# Print the length of tokenized words\n",
    "print(\"Length of Tokenized Words:\", len(tokenized_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4bccd6",
   "metadata": {},
   "source": [
    "**Expected output 133**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a2cb6",
   "metadata": {},
   "source": [
    "#### Tokenizing the Sentences and provide the length  (use banglanltk library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3723ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenize\n",
    "\n",
    "tokenized_sentences = banglanltk.sent_tokenize(paragraph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "adc134cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Tokenized sentences: 12\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Sentences length\n",
    "print(\"Length of Tokenized sentences:\", len(tokenized_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb6af5",
   "metadata": {},
   "source": [
    "**Expected output 12**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a26a91",
   "metadata": {},
   "source": [
    "#### Apply stopwords and stemmer in the tokenized sentences.\n",
    "###### Use stopwords from \"from nltk.corpus import stopwords\" and stremmer from  \"banglanltk\" package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24d28ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import banglanltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Download the necessary resources for stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = banglanltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Get Bengali stopwords from nltk.corpus\n",
    "stop_words = set(stopwords.words(\"bengali\"))\n",
    "\n",
    "# Apply stopwords and stemmer\n",
    "filtered_sentences = []\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    # Tokenize words in the sentence\n",
    "    words = banglanltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    \n",
    "    # Join the stemmed words to form a sentence\n",
    "    filtered_sentence = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Append the filtered sentence to the list\n",
    "    filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "# Print the filtered sentences\n",
    "#print(\"Filtered Sentences:\")\n",
    "#for i, sentence in enumerate(filtered_sentences, start=1):\n",
    "    #print(f\"{i}. {sentence}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd7b8e",
   "metadata": {},
   "source": [
    "#### Again tokenize the words in sentences that get after applying stopwords and stemming. And provide the length of the tokenize words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f6d986dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# word tokenize\n",
    "\n",
    "# Download the necessary resources for stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = banglanltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Get Bengali stopwords from nltk.corpus\n",
    "stop_words = set(stopwords.words(\"bengali\"))\n",
    "\n",
    "# Apply stopwords and stemmer\n",
    "filtered_sentences = []\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    # Tokenize words in the sentence\n",
    "    words = banglanltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    \n",
    "    # Join the stemmed words to form a sentence\n",
    "    filtered_sentence = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Append the filtered sentence to the list\n",
    "    filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "# Tokenize words in the filtered sentences\n",
    "tokenized_words = [word for sentence in filtered_sentences for word in banglanltk.word_tokenize(sentence)]\n",
    "\n",
    "# Print the tokenized words and their length\n",
    "#print(\"Tokenized Words:\", tokenized_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fedeb7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokenized Words: 111\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Words length\n",
    "print(\"Number of Tokenized Words:\", len(tokenized_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c50539",
   "metadata": {},
   "source": [
    "**Expected output 111**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705a5f7",
   "metadata": {},
   "source": [
    "### Stemmer Vs Lemamtization\n",
    "Stemming and lemmatization are both techniques used in natural language processing (NLP) and text mining to reduce words to their base or root form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6914508",
   "metadata": {},
   "source": [
    "# What are the differences between Stemmer and Lemmatization(Provive atleast 2 points)\n",
    "Stemming: The output of a stemmer is a root or base form of a word, which may not always be a valid word in the language. Stemmers use heuristics to remove prefixes or suffixes, aiming to reduce words to a common base form. For example, the stem of \"running\" is \"run.\"\n",
    "Lemmatization: The output of lemmatization, on the other hand, is a valid word (lemma) in the language. Lemmatization considers the meaning of the word and applies morphological analysis to produce the base form. For example, the lemma of \"running\" is \"run.\"\n",
    "Stemming: Stemming is a more aggressive approach, and it may result in the reduction of words that are not actually morphological variants. It is faster but may sacrifice precision for speed. For example, \"happiness\" and \"happy\" both stem to \"happi,\" even though their meanings are closely related but not identical.\n",
    "\n",
    "Lemmatization: Lemmatization considers the context and meaning of a word. It requires access to a lexicon or a knowledge base to understand the word's part of speech and morphological variations. Lemmatization provides a more accurate representation of a word's base form and is suitable for applications where precision and context preservation are crucial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ffe090",
   "metadata": {},
   "source": [
    "Suppose there are some application where we will use Stemming and Lemmatization text processing technique. Tell Us which text processing technique suitable for these applications. Provide answer like **(# Content Filtering -> Stemming)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "11f2fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis -> Lemmatization\n",
    "# Chatbots application -> Stemming\n",
    "# Gmail spam classification -> Stemming\n",
    "# Question answer -> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff066d",
   "metadata": {},
   "source": [
    "### Create Bag of words\n",
    "**1.** Step 1: Cleaning the text that not contain any numeric value, any punctuations etc.(In the give text)\n",
    "\n",
    "**2.** Step 2: Apply stopwords and stemmer again in the clean data.\n",
    "\n",
    "**3.** Step 3: Create Bag of Words (Use \"from sklearn.feature_extraction.text import CountVectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61dd618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Bag of Words (BoW):\n",
      "['অনয', 'অপর', 'অপরবরতত', 'অবসথন', 'আগর', 'একসচঞজ', 'এছড', 'কট', 'কম', 'কমছ', 'কমপন', 'করছ', 'গতকল', 'চটটগরম', 'টক', 'টকর', 'টর', 'ডএস', 'ডএসই', 'ডএসইএকস', 'ডএসইএস', 'ডএসইত', 'থক', 'দখ', 'দডয়ছ', 'দন', 'দর', 'দশর', 'পরধন', 'পয়নট', 'ফনডর', 'বজর', 'বড', 'বডছ', 'বশলষণ', 'মউচয়ল', 'মঙগলবর', 'মধয', 'যয়', 'রয়ছ', 'লখ', 'লনদন', 'শরয়হ', 'শয়র', 'শয়রবজর', 'সএসই', 'সএসপআই', 'সচক', 'সচকগলর', 'সটক', 'সরবক', 'হজর', 'হয়ছ', 'হয়ছল']\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Given paragraph\n",
    "paragraph = \"গতকাল ডিএসইতে ১ হাজার ১৯০ কোটি ২৬ লাখ টাকার শেয়ার লেনদেন হয়েছে। যা আগের দিন থেকে ৫ কোটি ১৫ লাখ টাকা কম! গতকাল ডি.এসইতে ১ হাজার ১৯৫ কোটি ৪১ লাখ টাকার শেয়ার লেনদেন হয়েছিল। এর আগের দিন মঙ্গলবার ১ হাজার ১৮৩ কোটি টাকা শেয়ার লেনদেন হয়েছিল। বাজার বিশ্লেষণে দেখা যায় ডিএসই প্রধান সূচক ডিএসইএক্স ১২ পয়েন্ট বেড়ে অবস্থান করছে ৬ হাজার ৩১২ পয়েন্টে। অন্য সূচকগুলোর মধ্যে ডিএসইএস বা শরিয়াহ সূচক ৪ পয়েন্ট বেড়ে অবস্থান করছে ১ হাজার ৩৭৫ পয়েন্টে। এছাড়া ডিএস৩০ সূচক ৬ পয়েন্ট বেড়ে দাঁড়িয়েছে ২ হাজার ২৬৫ পয়েন্টে। দেশের প্রধান এই শেয়ারবাজারে গতকাল ৩৮১টি কোম্পানি ও মিউচুয়াল ফান্ডের শেয়ার লেনদেন হয়েছে। এর মধ্যে দর বেড়েছে ১২৯টির। কমেছে ১৯৯টির এবং অপরিবর্তিত রয়েছে ৫৩টির। অপর বাজার চট্টগ্রাম স্টক  এক্সচেঞ্জে সিএসই সিএসই সার্বিক সূচক সিএসপিআই ৭৮ point বেড়েছে। লেনদেন হয়েছে 21 core টাকার শেয়ার।\"\n",
    "\n",
    "# Step 1: Cleaning the text\n",
    "cleaned_text = re.sub(r'[^\\w\\s]', '', paragraph)\n",
    "cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "cleaned_text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', cleaned_text)\n",
    "#cleaned_text = re.sub(\"[\\u0980-\\u09FF']+\", '' , cleaned_text)\n",
    "\n",
    "# Step 2: Apply stopwords and stemmer\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "tokenized_words = cleaned_text.split()\n",
    "filtered_words = [ps.stem(word) for word in tokenized_words if word.lower() not in stop_words]\n",
    "\n",
    "# Step 3: Create Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([\" \".join(filtered_words)])\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Display the Bag of Words\n",
    "print(\"Bag of Words (BoW):\")\n",
    "print(feature_names)\n",
    "#print(X.toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba0866",
   "metadata": {},
   "source": [
    "###### Expected output\n",
    "array(['অপর', 'অপরিবর্তিত', 'অবস্হা', 'আগের', 'এক্সচেঞ্জ', 'এছাড়া', 'কম',\n",
    "       'কমেছ', 'কে', 'কোম্', 'গতকাল', 'চট্টগ্রাম', 'টাকা', 'টাকার',\n",
    "       'ডিএস', 'ডিএসই', 'ডিএসইএক্স', 'ডিএসইএস', 'ডিএসইত', 'দর', 'দাঁড়',\n",
    "       'দেশের', 'প্রধা', 'পয়েন্ট', 'ফান্ডের', 'বাজার', 'বিশ্লেষণ', 'বেড়',\n",
    "       'বেড়েছ', 'মঙ্গলবার', 'মিউচুয়াল', 'যায়', 'রয়', 'লাখ', 'লেনদে',\n",
    "       'শরিয়াহ', 'শেয়ার', 'শেয়ারবাজার', 'সার্ব', 'সিএসই', 'সিএসপিআই',\n",
    "       'সূচক', 'সূচকগুলোর', 'স্টক', 'হয়'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8099914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce924e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
