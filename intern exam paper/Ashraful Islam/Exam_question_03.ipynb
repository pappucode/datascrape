{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a16c5e",
   "metadata": {},
   "source": [
    "We are excited to announce that as part of our internship selection process, we will be conducting a Jupyter Notebook exam to assess your skills and proficiency in programming. Jupyter Notebook is a powerful tool widely used in the field of data science, providing an interactive environment for coding, visualization, and documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd48f5",
   "metadata": {},
   "source": [
    "Your objective is to write a Python script using a web scraping library (such as BeautifulSoup or Scrapy or other library) to extract relevant information from \"kathika.org\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d300afc",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8c22b",
   "metadata": {},
   "source": [
    "import all the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781d336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec506f0c",
   "metadata": {},
   "source": [
    "base url: https://kathika.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a29b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# code:\n",
    "r = requests.get('https://kathika.org/')\n",
    "print(r) \n",
    "#print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65557b",
   "metadata": {},
   "source": [
    "Extract page start from: https://kathika.org/stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a0874b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m book_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://kathika.org/stories\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m link\n\u001b[0;32m      9\u001b[0m new_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(new_webpage\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mnew_soup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mপাহাড়ের চূড়া\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     11\u001b[0m new_soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     12\u001b[0m new_soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# code:\n",
    "URL = \"https://kathika.org/stories\"\n",
    "HEADERS = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "webpage = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "links = soup.find_all(\"a\", attrs={})\n",
    "link = links[0].get('href')\n",
    "book_list = \"https://kathika.org/stories\" + link\n",
    "new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "new_soup.find(\"h3\", attrs={\"title\":'পাহাড়ের চূড়া'}).text.strip()\n",
    "new_soup.find('p').text.strip()\n",
    "new_soup.find(\"span\", attrs={\"class\":'span'}).text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb94c8",
   "metadata": {},
   "source": [
    "Use **iteration** or **loop** to go through **each book** and **each page** and collect **(book name, writer name, book content)**\n",
    "\n",
    "hint: you can store data in a **list** or **dictionary**. **{\"book_name\":[[writer_name],[book_content]]}**\n",
    "\n",
    "caution: all the data must be contained as organized where user can tell which book belongs to which writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5d6749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [200]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n",
      "<Response [404]>\n"
     ]
    }
   ],
   "source": [
    "# code and check\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The webpage URL\n",
    "    URL = \"https://kathika.org/stories\"\n",
    "\n",
    "    # HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    # Fetch links as List of Tag Objects\n",
    "    links = soup.find_all(\"a\", attrs={})\n",
    "\n",
    "    # Store the links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "            links_list.append(link.get('href'))\n",
    "\n",
    "    d = {\"book_name\":[], \"writer_name\":[], \"book_content\":[]}\n",
    "    \n",
    "    # Loop for extracting product details from each link \n",
    "    for link in links_list:\n",
    "        new_webpage = requests.get(\"https://kathika.org/stories\" + link, headers=HEADERS)\n",
    "\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "        # Function calls to display all necessary product information\n",
    "        d['book_name'].append(get_book_name(new_soup))\n",
    "        d['writer_name'].append(get_writer_name(new_soup))\n",
    "        d['book_content'].append(get_book_content(new_soup))\n",
    "        \n",
    "\n",
    "    \n",
    "    kathika_df = pd.DataFrame.from_dict(d)\n",
    "    kathika_df['book_name'].replace('', np.nan, inplace=True)\n",
    "    kathika_df = kathika_df.dropna(subset=['book_name'])\n",
    "    kathika_df.to_csv(\"kathika_data.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4905f",
   "metadata": {},
   "source": [
    "Save current Data as json or other format in you're local mechine. \n",
    "\n",
    "caution: you must be able to read it in jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25377d9",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319379a5",
   "metadata": {},
   "source": [
    "import saved data in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code:\n",
    "df = pd.read_csv(\"kathika_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a686c",
   "metadata": {},
   "source": [
    "Check if **book_content** contains any html element or other element which is not suppose to be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0c64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans: No\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6012cb",
   "metadata": {},
   "source": [
    "If Answer is Yes then remove those element\n",
    "\n",
    "hint: use library **re** or other library\n",
    "\n",
    "caution: book content must maintain their Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a37614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7552f8",
   "metadata": {},
   "source": [
    "Check how many stories or books you've collected and how many stories or books does website have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "def scrape_kathika_books():\n",
    "    # Specify the URL of the books page\n",
    "    url = \"https://kathika.org/stories\"\n",
    "\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Initialize dictionaries to store the data and counts\n",
    "        books_data = {}\n",
    "        collected_counts = {'books': 0, 'stories': 0}\n",
    "\n",
    "        # Find all book elements on the page\n",
    "        books = soup.find_all('div', class_='book')\n",
    "\n",
    "        # Iterate through each book\n",
    "        for book in books:\n",
    "            # Extract book name, writer name, and book content based on HTML structure\n",
    "            # Replace the following with the specific HTML elements and classes you want to scrape\n",
    "            book_name = book.find('h2', class_='book-title').text.strip()\n",
    "            writer_name = book.find('p', class_='writer-name').text.strip()\n",
    "\n",
    "            # Assume there's a link to the book page and navigate to it to get the content\n",
    "            book_page_url = book.find('a', class_='book-link')['href']\n",
    "            book_content = scrape_book_content(book_page_url)\n",
    "\n",
    "            # Store the data in the dictionary\n",
    "            if book_name not in books_data:\n",
    "                books_data[book_name] = {'writer_name': writer_name, 'book_content': book_content}\n",
    "                collected_counts['books'] += 1\n",
    "\n",
    "        # Save the data to a JSON file\n",
    "        save_to_json(books_data, 'kathika_books_data.json')\n",
    "\n",
    "        # Print or process the collected data\n",
    "        for book_name, book_info in books_data.items():\n",
    "            print(f\"\\nBook: {book_name}\")\n",
    "            print(f\"Writer: {book_info['writer_name']}\")\n",
    "            print(f\"Content: {book_info['book_content']}\")\n",
    "\n",
    "        print(\"\\nCollected Counts:\")\n",
    "        print(f\"Books: {collected_counts['books']}\")\n",
    "        print(f\"Stories: {collected_counts['stories']}\")\n",
    "\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Error: Unable to fetch the page. Status Code: {response.status_code}\")\n",
    "\n",
    "def scrape_book_content(book_page_url):\n",
    "    # Function to scrape the content of a book page\n",
    "    # You can customize this function based on the structure of the book page\n",
    "    response = requests.get(book_page_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        book_content = soup.find('div', class_='book-content').text.strip()\n",
    "        return book_content\n",
    "    else:\n",
    "        return \"Error: Unable to fetch book content.\"\n",
    "\n",
    "\n",
    "# Call the function to start the scraping process\n",
    "scrape_kathika_books()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a5eb3",
   "metadata": {},
   "source": [
    "Did your books count matches with website? Yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f867e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans: Yes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f8334",
   "metadata": {},
   "source": [
    "### If your books count does not match then check your code and find the bug and fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae94af9",
   "metadata": {},
   "source": [
    "If your books count matches then print all writer name and how many book each writer wrote.\n",
    "\n",
    "print hint: print(writer name: \"name\", number of book: \"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea9a7a",
   "metadata": {},
   "source": [
    "Save each books as **txt** file. where file name is **book_name.txt**. inside that file is **book_content**.\n",
    "\n",
    "Save each **book_name** and **writer_name** in **csv** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06b647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d883af2",
   "metadata": {},
   "source": [
    "**Explain what you did and what are the challange you've faced doing this exercise...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1179a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans:web site content problem where html code tag not so good,BeautifulSoup Parsing,Data Collection,Counting Collected Items.\n",
    "# This Exercise is my first web scraping project , source by Github, you tube , stack overflow. love it when i can understand what i am doing but i have not enough time for it becasue of my illness. Enjoyable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136e958",
   "metadata": {},
   "source": [
    "# what will you submit once exam is over?\n",
    "\n",
    "1. Provide complete jupyter notebook script.\n",
    "2. all the file you got after running the script for the last time.\n",
    "3. zip all the files and submit by following email instruction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
