{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7369df4d",
   "metadata": {
    "id": "7369df4d"
   },
   "source": [
    "We are excited to announce that as part of our internship selection process, we will be conducting a Jupyter Notebook exam to assess your skills and proficiency in programming. Jupyter Notebook is a powerful tool widely used in the field of data science, providing an interactive environment for coding, visualization, and documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932767a",
   "metadata": {
    "id": "2932767a"
   },
   "source": [
    "Your objective is to write a Python script using a web scraping library (such as BeautifulSoup or Scrapy or other library) to extract relevant information from \"www.pcmag.com\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b2fcb",
   "metadata": {
    "id": "f15b2fcb"
   },
   "source": [
    "## Data scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173c5a55",
   "metadata": {
    "id": "173c5a55"
   },
   "source": [
    "import all the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59399c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e59399c0",
    "outputId": "2686d520-0453-4e35-9cd2-a416ab065ab0"
   },
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4\n",
    "!pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee5fc0",
   "metadata": {
    "id": "aaee5fc0"
   },
   "source": [
    "base url: https://www.pcmag.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1vwV4orgJYU",
   "metadata": {
    "id": "h1vwV4orgJYU"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69cffa83",
   "metadata": {
    "id": "69cffa83"
   },
   "source": [
    "Extract page start from: https://www.pcmag.com/news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996c145b",
   "metadata": {
    "id": "996c145b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the page. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://www.pcmag.com\"\n",
    "\n",
    "# URL to start scraping from\n",
    "start_url = \"https://www.pcmag.com/news\"\n",
    "\n",
    "# Function to fetch and parse the webpage\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract relevant information\n",
    "def extract_info_from_page(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Assuming we want to extract titles of news articles, you can adjust this based on the actual data you want\n",
    "    titles = []\n",
    "    for title_tag in soup.select('.headline-list-title'):\n",
    "        titles.append(title_tag.text.strip())\n",
    "\n",
    "    return titles\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    content = fetch_page_content(start_url)\n",
    "    if content:\n",
    "        extracted_data = extract_info_from_page(content)\n",
    "        for idx, title in enumerate(extracted_data, start=1):\n",
    "            print(f\"{idx}. {title}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9c5f9",
   "metadata": {
    "id": "a2e9c5f9"
   },
   "source": [
    "**There are over 2000+ pages of Data in this site.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b304150",
   "metadata": {
    "id": "3b304150"
   },
   "source": [
    "**task is to collect everything listed below:**\n",
    "\n",
    "**1.** News Title\n",
    "\n",
    "**2.** Publisher Name\n",
    "\n",
    "**3.** Publish date and time.\n",
    "\n",
    "**4.** Source Link.\n",
    "\n",
    "**5.** News content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfecf5",
   "metadata": {
    "id": "1dcfecf5"
   },
   "source": [
    "### Warning: Collect data from first 200 pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb80687",
   "metadata": {
    "id": "0bb80687"
   },
   "source": [
    "## Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a38a5c",
   "metadata": {
    "id": "85a38a5c"
   },
   "source": [
    "Use **iteration** or **loop** to go through **each news** and **each page** and collect **(News Title,  Publisher Name, Publish date and time, Source Link, News content)**\n",
    "\n",
    "hint: you can store data in a **DataFrame(pandas)** OR your **preferred library**.\n",
    "\n",
    "caution: all the data must be contained as **organized** where user can tell which news belongs to which publisher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f32db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a5f32db",
    "outputId": "d25bd93a-c4a9-481e-9342-85cf87f23c3c"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = \"https://www.pcmag.com/news\"\n",
    "MAX_PAGES = 200\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def extract_news_from_page(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    news_list = []\n",
    "\n",
    "    articles = soup.select('.reviewpage-articles > article')\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.select_one('.headline-list-title').text.strip()\n",
    "        publisher = article.select_one('.byline > .byline-author').text.strip()\n",
    "        date_time = article.select_one('.byline > .byline-date').text.strip()\n",
    "        source_link = BASE_URL + article.select_one('.headline-list-title > a')['href']\n",
    "        content = article.select_one('.reviewpage-content').text.strip()\n",
    "\n",
    "        news_list.append({\n",
    "            'News Title': title,\n",
    "            'Publisher Name': publisher,\n",
    "            'Publish Date and Time': date_time,\n",
    "            'Source Link': source_link,\n",
    "            'News Content': content\n",
    "        })\n",
    "\n",
    "    return news_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_news = []\n",
    "\n",
    "    for page_num in range(1, MAX_PAGES + 1):\n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "        page_url = f\"{BASE_URL}?page={page_num}\"\n",
    "        content = fetch_page_content(page_url)\n",
    "\n",
    "        if content:\n",
    "            news_on_page = extract_news_from_page(content)\n",
    "            all_news.extend(news_on_page)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_news)\n",
    "\n",
    "    # Save to CSV (optional)\n",
    "    df.to_csv('pcmag_news_data.csv', index=False)\n",
    "\n",
    "    print(\"Data collection completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58032943",
   "metadata": {
    "id": "58032943"
   },
   "source": [
    "Sort your Data accoding to latest **publish_date_and_time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58feadfb",
   "metadata": {
    "id": "58feadfb"
   },
   "outputs": [],
   "source": [
    "# ... [rest of the code remains unchanged]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_news = []\n",
    "\n",
    "    for page_num in range(1, MAX_PAGES + 1):\n",
    "        print(f\"Scraping page {page_num}...\")\n",
    "        page_url = f\"{BASE_URL}?page={page_num}\"\n",
    "        content = fetch_page_content(page_url)\n",
    "\n",
    "        if content:\n",
    "            news_on_page = extract_news_from_page(content)\n",
    "            all_news.extend(news_on_page)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_news)\n",
    "\n",
    "    # Sort by Publish Date and Time\n",
    "    df_sorted = df.sort_values(by='Publish Date and Time', ascending=False)\n",
    "\n",
    "    # Save to CSV (optional)\n",
    "    df_sorted.to_csv('pcmag_news_data_sorted.csv', index=False)\n",
    "\n",
    "    print(\"Data collection and sorting completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010ed58",
   "metadata": {
    "id": "c010ed58"
   },
   "source": [
    "Save current **Dataframe** as csv in you're local mechine.\n",
    "\n",
    "caution: you must be able to read it in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56075397",
   "metadata": {
    "id": "56075397"
   },
   "outputs": [],
   "source": [
    "#code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a6654",
   "metadata": {
    "id": "e77a6654"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57c894",
   "metadata": {
    "id": "aa57c894"
   },
   "source": [
    "import saved data in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628aae98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "628aae98",
    "outputId": "83211bb6-1cb6-45fa-d047-f54bf6613d23"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_csv('pcmag_news_data_sorted.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62899c8b",
   "metadata": {
    "id": "62899c8b"
   },
   "source": [
    "Check if **News_content** contains any **html element** or **other element** which is not suppose to be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93a908",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "be93a908",
    "outputId": "518f1b37-9b61-405f-f52e-f4137fca2c79"
   },
   "outputs": [],
   "source": [
    "# Ans: Yes or No\n",
    "import re\n",
    "\n",
    "def contains_html(content):\n",
    "    return bool(re.search('<.*?>', content))\n",
    "\n",
    "df['Contains HTML'] = df['News Content'].apply(contains_html)\n",
    "\n",
    "# Check if any News Content contains HTML\n",
    "if df['Contains HTML'].any():\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a3fd6e",
   "metadata": {
    "id": "92a3fd6e"
   },
   "source": [
    "If Answer is Yes then remove those element\n",
    "\n",
    "hint: use library **re** or other library\n",
    "\n",
    "caution: **News_content** must maintain their Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1f695",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "c8f1f695",
    "outputId": "46341bce-170a-4584-cc9c-9e4e5e4aa8ac"
   },
   "outputs": [],
   "source": [
    "# code:\n",
    "def remove_html_tags(content):\n",
    "    return re.sub('<.*?>', '', content)\n",
    "\n",
    "df['Cleaned News Content'] = df['News Content'].apply(remove_html_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9078093",
   "metadata": {
    "id": "f9078093"
   },
   "source": [
    "Check how many **News Info** you've collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a75132",
   "metadata": {
    "id": "c7a75132"
   },
   "outputs": [],
   "source": [
    "#code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad6af9f",
   "metadata": {
    "id": "1ad6af9f"
   },
   "source": [
    "Export the cleaned **DataFrame** into your local mechine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bbb3a",
   "metadata": {
    "id": "b86bbb3a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d630b14",
   "metadata": {
    "id": "2d630b14"
   },
   "source": [
    "Explain what you did and what are the challange you've faced doing this exercise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f24e9",
   "metadata": {
    "id": "e71f24e9"
   },
   "outputs": [],
   "source": [
    "# Ans:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccef4c2",
   "metadata": {
    "id": "9ccef4c2"
   },
   "source": [
    "# what will you submit once exam is over?\n",
    "\n",
    "1. Provide complete jupyter notebook script.\n",
    "2. all the file you got after running the script for the last time.\n",
    "3. zip all the files and submit by following email instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cpt0x5wWbNMq",
   "metadata": {
    "id": "cpt0x5wWbNMq"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
