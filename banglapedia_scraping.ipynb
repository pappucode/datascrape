{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59b88af",
   "metadata": {},
   "source": [
    "# Banglapedia full scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d8b0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5bfa4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = requests.get(\"https://bn.banglapedia.org/index.php?\")\n",
    "soup = bs(source.text, 'lxml')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8343202",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_table = soup.find('div', class_='table-responsive')\n",
    "#print(letter_table.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "feedf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all table cells with links\n",
    "table_cells = letter_table.find_all('td')\n",
    "\n",
    "# Extract text and href attributes\n",
    "for cell in table_cells:\n",
    "    link = cell.find('a')  # Find the <a> tag inside the <td>\n",
    "    if link:\n",
    "        #text = link.get_text(strip=True)  # Get the text inside the <a>\n",
    "        href = link['href']  # Get the href attribute of the <a>\n",
    "        href = f\"https://bn.banglapedia.org{href}\"\n",
    "        #print(f\"{text}\")\n",
    "        #print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a486fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://bn.banglapedia.org/index.php?title=%E0%A6%95%E0%A6%B0: ('Connection aborted.', ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None))\n",
      "Successfully scraped 14416 out of 14417 links in 1008.19 seconds\n",
      "Collected 14417 links so far.\n"
     ]
    }
   ],
   "source": [
    "# Function to process a single link\n",
    "def process_link(link_data):\n",
    "    text, full_link, session = link_data\n",
    "    try:\n",
    "        # Fetch content for the link using the session\n",
    "        response = session.get(full_link)\n",
    "        content_soup = bs(response.text, 'lxml')\n",
    "\n",
    "        # Extract the text content\n",
    "        text_content = content_soup.find('div', class_='mw-parser-output').get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # Save to a file\n",
    "        file_path = f\"banglapedia/{text}.txt\"\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)  # Ensure the directory exists\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text_content)\n",
    "\n",
    "        return True  # Indicates success\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing link {full_link}: {e}\")\n",
    "        return False  # Indicates failure\n",
    "\n",
    "# Main script\n",
    "source = requests.get(\"https://bn.banglapedia.org/index.php?\")\n",
    "soup = bs(source.text, 'lxml')\n",
    "letter_table = soup.find('div', class_='table-responsive')\n",
    "\n",
    "# Find all table cells with links\n",
    "table_cells = letter_table.find_all('td')\n",
    "\n",
    "# Extract text and href attributes\n",
    "links_data = []\n",
    "for cell in table_cells:\n",
    "    link = cell.find('a')  # Find the <a> tag inside the <td>\n",
    "    if link:\n",
    "        #text = link.get_text(strip=True)  # Get the text inside the <a>\n",
    "        href = link['href']  # Get the href attribute of the <a>        \n",
    "        href = f\"https://bn.banglapedia.org{href}\"\n",
    "        #print(f\"{text}\")\n",
    "        #print(href)\n",
    "        \n",
    "        source1 = requests.get(href)\n",
    "        soup1 = bs(source1.text, 'lxml')\n",
    "\n",
    "        container = soup1.find('div', class_='mw-body-content')\n",
    "        link_container = container.find_all('li')\n",
    "\n",
    "        # Extract text and href attributes\n",
    "        for url in link_container:\n",
    "            link = url.find('a')  # Find the <a> tag inside the <li>\n",
    "            if link:\n",
    "                text = link.get_text(strip=True)  # Get the text inside the <a>\n",
    "                href = link['href']  # Get the href attribute of the <a>\n",
    "                full_link = f\"https://bn.banglapedia.org{href}\"\n",
    "                links_data.append((text, full_link))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use ThreadPoolExecutor with a shared requests.Session\n",
    "with requests.Session() as session:\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        # Include the session in the link data\n",
    "        results = list(executor.map(process_link, [(text, full_link, session) for text, full_link in links_data]))\n",
    "\n",
    "# Calculate the total links processed successfully\n",
    "successful_links = sum(results)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"Successfully scraped {successful_links} out of {len(links_data)} links in {duration:.2f} seconds\")\n",
    "print(f\"Collected {len(links_data)} links so far.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc97b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without using session and multithreading\n",
    "  #--> Scraped 345 links of content in 322.02347707748413 seconds\n",
    "# Using only session \n",
    "  #-->Scraped 345 links of content in 327.3671143054962 seconds\n",
    "# Using only ThreadPoolExecutor\n",
    "  #-->Successfully scraped 345 out of 345 links in 114.21 seconds\n",
    "# Using ThreadPoolExecutor and session\n",
    "  #-->Successfully scraped 345 out of 345 links in 16.67 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source1 = requests.get(\"https://bn.banglapedia.org/index.php?title=বিশেষ:সব_পাতা/অ\")   \n",
    "soup1 = bs(source1.text, 'lxml')\n",
    "#rint(soup1.prettify())\n",
    "\n",
    "container= soup1.find('div', class_='mw-body-content')\n",
    "#print(link_container.prettify())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "link_container = container.find_all('li')\n",
    "\n",
    "total_links = 0\n",
    "# Extract text and href attributes\n",
    "for url in link_container:\n",
    "    link = url.find('a')  # Find the <a> tag inside the <td>\n",
    "    if link:\n",
    "        text = link.get_text(strip=True)  # Get the text inside the <a>\n",
    "        href = link['href']  # Get the href attribute of the <a>\n",
    "        full_link = f\"https://bn.banglapedia.org{href}\"\n",
    "        total_links += 1 \n",
    "        #print(f\"{text}\")\n",
    "        #print(f\"{full_link}\")\n",
    "#print(f\"Total links scraped: {total_links}\")\n",
    "       \n",
    "        content_page = requests.get(full_link)\n",
    "        content_soup =  bs(content_page.text, 'lxml')\n",
    "        #print(content_soup.prettify())\n",
    "        text_content = content_soup.find('div', class_='mw-parser-output').get_text(separator='\\n', strip=True)\n",
    "        #print(text_content)\n",
    "\n",
    "        file_path = f\"banglapedia/{text}.txt\"\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text_content)\n",
    "            \n",
    "duration = time.time() - start_time        \n",
    "print(f\"Scraped {total_links} links of content in {duration} seconds\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dafed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single link\n",
    "def process_link(link_data):\n",
    "    \n",
    "    text, full_link, session = link_data\n",
    "    try:\n",
    "        # Fetch content for the link using the session\n",
    "        response = session.get(full_link)\n",
    "        content_soup = bs(response.text, 'lxml')\n",
    "\n",
    "        # Extract the text content\n",
    "        text_content = content_soup.find('div', class_='mw-parser-output').get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # Save to a file\n",
    "        file_path = f\"banglapedia/{text}.txt\"\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)  # Ensure the directory exists\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text_content)\n",
    "\n",
    "        return True  # Indicates success\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing link {full_link}: {e}\")\n",
    "        return False  # Indicates failure\n",
    "\n",
    "# Main script\n",
    "source1 = requests.get(\"https://bn.banglapedia.org/index.php?title=বিশেষ:সব_পাতা/অ\")\n",
    "soup1 = bs(source1.text, 'lxml')\n",
    "\n",
    "container = soup1.find('div', class_='mw-body-content')\n",
    "link_container = container.find_all('li')\n",
    "\n",
    "# Extract text and href attributes\n",
    "links_data = []\n",
    "for url in link_container:\n",
    "    link = url.find('a')  # Find the <a> tag inside the <li>\n",
    "    if link:\n",
    "        text = link.get_text(strip=True)  # Get the text inside the <a>\n",
    "        href = link['href']  # Get the href attribute of the <a>\n",
    "        full_link = f\"https://bn.banglapedia.org{href}\"\n",
    "        links_data.append((text, full_link))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use ThreadPoolExecutor with a shared requests.Session\n",
    "with requests.Session() as session:\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Include the session in the link data\n",
    "        results = list(executor.map(process_link, [(text, full_link, session) for text, full_link in links_data]))\n",
    "\n",
    "# Calculate the total links processed successfully\n",
    "successful_links = sum(results)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"Successfully scraped {successful_links} out of {len(links_data)} links in {duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037665d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92311792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2089f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab786c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b8fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4c565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b3aa10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41bc0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a7791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
