{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27325b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d53164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file\n",
    "with open(\"home.html\", 'r') as html_file:\n",
    "    content = html_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400a8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(content)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2cb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python for beginners\n",
      "If you are new to Python, this is the course that you should buy!\n",
      "20$\n"
     ]
    }
   ],
   "source": [
    "# It will Extract only the single element\n",
    "article = soup.find(\"div\", class_ = \"card\")\n",
    "\n",
    "course_name = article.find(\"h5\", class_ = \"card-title\").text\n",
    "print(course_name)\n",
    "\n",
    "summary = article.find(\"p\", class_= \"card-text\").text\n",
    "print(summary)\n",
    "\n",
    "course_price = article.find(\"a\").text\n",
    "course_price = course_price.split()[-1]\n",
    "print(course_price)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786727d",
   "metadata": {},
   "source": [
    "### AttributeError: 'NavigableString' object has no attribute 'find_all'  \"What does it means\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84698f30",
   "metadata": {},
   "source": [
    "In BeautifulSoup, a NavigableString is a type of object that represents the textual content within HTML or XML tags. It's essentially the text enclosed within HTML tags.\n",
    "\n",
    "The find_all method is used to search for and extract specific elements (tags) from a BeautifulSoup object, such as a BeautifulSoup instance or a Tag object. However, you cannot use find_all on a NavigableString because it represents text, not HTML or XML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eacd62ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python for beginners\n",
      "If you are new to Python, this is the course that you should buy!\n",
      "20$\n",
      "\n",
      "Python Web Development\n",
      "If you feel enough confident with python, you are ready to learn how to create your own website!\n",
      "50$\n",
      "\n",
      "Python Machine Learning\n",
      "Become a Python Machine Learning master!\n",
      "100$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It will extract the multiple elements\n",
    "articles = soup.find_all(\"div\", class_ = \"card\")\n",
    "for article in articles:\n",
    "    course_name = article.find(\"h5\", class_ = \"card-title\").text\n",
    "    print(course_name)\n",
    "\n",
    "    summary = article.find(\"p\", class_= \"card-text\").text\n",
    "    print(summary)\n",
    "    \n",
    "    course_price = article.find(\"a\").text\n",
    "    course_price = course_price.split()[-1]\n",
    "    print(course_price)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19eaad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=\")\n",
    "#print(r)\n",
    "r = r.text\n",
    "#print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cbdcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r, 'lxml')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9a4649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li class=\"clearfix job-bx wht-shd-bx\">\n",
      " <header class=\"clearfix\">\n",
      "  <!--\n",
      "-->\n",
      "  <!-- -->\n",
      "  <h2>\n",
      "   <a href=\"https://www.timesjobs.com/job-detail/python-engineer-east-india-securities-ltd-kolkata-2-to-5-yrs-jobid-KEkE19WqPbFzpSvf__PLUS__uAgZw==&amp;source=srp\" onclick=\"logViewUSBT('view','66865756','python  ,  hadoop  ,  machine learning','Kolkata','2 - 5','IT Software : Software Products &amp; Services','1','' )\" target=\"_blank\">\n",
      "    <strong class=\"blkclor\">\n",
      "     Python\n",
      "    </strong>\n",
      "    Engineer\n",
      "   </a>\n",
      "  </h2>\n",
      "  <h3 class=\"joblist-comp-name\">\n",
      "   east india securities ltd.\n",
      "  </h3>\n",
      " </header>\n",
      " <ul class=\"top-jd-dtl clearfix\">\n",
      "  <li>\n",
      "   <i class=\"material-icons\">\n",
      "    card_travel\n",
      "   </i>\n",
      "   2 - 5 yrs\n",
      "  </li>\n",
      "  <li>\n",
      "   <i class=\"material-icons\">\n",
      "    location_on\n",
      "   </i>\n",
      "   <span title=\"Kolkata\">\n",
      "    Kolkata\n",
      "   </span>\n",
      "  </li>\n",
      " </ul>\n",
      " <ul class=\"list-job-dtl clearfix\">\n",
      "  <li>\n",
      "   <label>\n",
      "    Job Description:\n",
      "   </label>\n",
      "   job_description   2  years of experience working with python    Strong analytical skills    Ability to churn scripting solutions with a quick turnaround time    Inclination to...\n",
      "   <a href=\"https://www.timesjobs.com/job-detail/python-engineer-east-india-securities-ltd-kolkata-2-to-5-yrs-jobid-KEkE19WqPbFzpSvf__PLUS__uAgZw==&amp;source=srp\" target=\"_blank\">\n",
      "    More Details\n",
      "   </a>\n",
      "  </li>\n",
      "  <li>\n",
      "   <label>\n",
      "    KeySkills:\n",
      "   </label>\n",
      "   <span class=\"srp-skills\">\n",
      "    <strong class=\"blkclor\">\n",
      "     python\n",
      "    </strong>\n",
      "    ,  hadoop  ,  machine learning\n",
      "   </span>\n",
      "  </li>\n",
      "  <!--\n",
      "            <li>\n",
      "              <i class=\"material-icons\">location_on</i>\n",
      "              Kolkata\n",
      "              </li>\n",
      "-->\n",
      " </ul>\n",
      " <div class=\"list-job-bt clearfix\">\n",
      "  <div class=\"list-action\">\n",
      "   <div class=\"applied-dtl clearfix\" id=\"showPostApplyData_66865756\">\n",
      "    <a class=\"waves-effect waves-light btn\" href=\"javascript:callExtJobApply('66865756','adId=KEkE19WqPbFzpSvf__PLUS__uAgZw==&amp;compName=Career Progress Consultants','TJPFSRP');\" onclick=\"trackClickEvent('View_AND_Apply_SRP','from_srp_externalJobs');logViewUSBT('apply','66865756','python  ,  hadoop  ,  machine learning','Kolkata','2 - 5','IT Software : Software Products &amp; Services','1','')\">\n",
      "     Apply\n",
      "    </a>\n",
      "    <span class=\"jobs-status clearfix\">\n",
      "     <!--\n",
      "-->\n",
      "    </span>\n",
      "    <span class=\"sim-posted\">\n",
      "     <span>\n",
      "      Posted 3 days ago\n",
      "     </span>\n",
      "    </span>\n",
      "   </div>\n",
      "  </div>\n",
      " </div>\n",
      "</li>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = soup.find(\"li\", class_ = \"clearfix job-bx wht-shd-bx\")\n",
    "print(job.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d4bf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Engineer\n"
     ]
    }
   ],
   "source": [
    "job_name = job.find(\"h2\").text.strip()\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674cf888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "east india securities ltd.\n"
     ]
    }
   ],
   "source": [
    "company_name = job.find(\"h3\", class_ = \"joblist-comp-name\").text.strip()\n",
    "company_name = re.sub(r\"\\s*\\(\\s*\", \" (\", company_name)\n",
    "company_name = re.sub(r\"\\s*\\)\\s*\", \")\", company_name)\n",
    "print(company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5498720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python, hadoop, machine learning\n"
     ]
    }
   ],
   "source": [
    "skills = job.find(\"span\", class_ = \"srp-skills\").text.strip()\n",
    "skills = re.sub(r\"\\s*,\\s*\", ', ', skills)\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bb8cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.timesjobs.com/job-detail/python-engineer-east-india-securities-ltd-kolkata-2-to-5-yrs-jobid-KEkE19WqPbFzpSvf__PLUS__uAgZw==&source=srp\n"
     ]
    }
   ],
   "source": [
    "more_info = job.header.h2.a['href']\n",
    "print(more_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4d4187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posted 3 days ago\n"
     ]
    }
   ],
   "source": [
    "published_date = job.find(\"span\", class_=\"sim-posted\").span.text\n",
    "print(published_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2cfc8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put some skill that you are not familiar with\n",
      ">\n",
      "Filtering out \n"
     ]
    }
   ],
   "source": [
    "#Take Some Input\n",
    "print('Put some skill that you are not familiar with')\n",
    "unfamiliar_skill = input('>')\n",
    "print(f\"Filtering out {unfamiliar_skill}\")\n",
    "\n",
    "# Combine them all togather\n",
    "def find_jobs():\n",
    "    \n",
    "    jobs = soup.find_all(\"li\", class_ = \"clearfix job-bx wht-shd-bx\")\n",
    "    \n",
    "    for index, job in enumerate(jobs):\n",
    "\n",
    "        published_date = job.find(\"span\", class_=\"sim-posted\").span.text\n",
    "        if 'few' in published_date:\n",
    "\n",
    "            job_name = job.find(\"h2\").text.strip()\n",
    "\n",
    "            company_name = job.find(\"h3\", class_ = \"joblist-comp-name\").text.strip()\n",
    "            company_name = re.sub(r\"\\s*\\(\\s*\", \" (\", company_name)\n",
    "            company_name = re.sub(r\"\\s*\\)\\s*\", \")\", company_name)\n",
    "\n",
    "            skills = job.find(\"span\", class_ = \"srp-skills\").text.strip()\n",
    "            skills = re.sub(r\"\\s*,\\s*\", ', ', skills)\n",
    "\n",
    "            more_info = job.header.h2.a['href']\n",
    "\n",
    "            if unfamiliar_skill not in skills:\n",
    "                with open(f'posts/{index}.txt', 'w') as f:\n",
    "                    f.write(f'{job_name}\\n')\n",
    "                    f.write(f'{company_name}\\n')\n",
    "                    f.write(f'{skills}\\n')\n",
    "                print(f'File Saved:{index}')\n",
    "#                     print(f\"\"\"\n",
    "#                     Job Name: {job_name}\n",
    "#                     Company Name: {company_name}\n",
    "#                     skills Set: {skills}\n",
    "#                     \"Job Link\": {more_info}\n",
    "#                     \"\"\")   \n",
    "# if __name__ == '__main__':\n",
    "#     while True:\n",
    "#         find_jobs()\n",
    "#         time_wait = 1\n",
    "#         print(f\"Waiting {time_wait} minutes...\")\n",
    "#         time.sleep(time_wait * 60)\n",
    "\n",
    "find_jobs()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78de796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put some skill that you are not familiar with\n",
      ">ii\n",
      "Filtering out ii\n",
      "File Saved:2\n",
      "File Saved:6\n",
      "File Saved:9\n",
      "File Saved:20\n",
      "File Saved:22\n",
      "File Saved:23\n",
      "File Saved:24\n"
     ]
    }
   ],
   "source": [
    "#Take Some Input\n",
    "print('Put some skill that you are not familiar with')\n",
    "unfamiliar_skill = input('>')\n",
    "print(f\"Filtering out {unfamiliar_skill}\")\n",
    "\n",
    "\n",
    "csv_file = open(\"job_info.csv\", 'w')\n",
    "csv_writter = csv.writer(csv_file)\n",
    "csv_writter.writerow(['Job name', 'Company name', 'Skills', 'More info'])\n",
    "\n",
    "def find_jobs():\n",
    "    \n",
    "    jobs = soup.find_all(\"li\", class_ = \"clearfix job-bx wht-shd-bx\")\n",
    "    \n",
    "    for index, job in enumerate(jobs):\n",
    "\n",
    "        published_date = job.find(\"span\", class_=\"sim-posted\").span.text\n",
    "        if 'few' in published_date:\n",
    "\n",
    "            job_name = job.find(\"h2\").text.strip()\n",
    "\n",
    "            company_name = job.find(\"h3\", class_ = \"joblist-comp-name\").text.strip()\n",
    "            company_name = re.sub(r\"\\s*\\(\\s*\", \" (\", company_name)\n",
    "            company_name = re.sub(r\"\\s*\\)\\s*\", \")\", company_name)\n",
    "\n",
    "            skills = job.find(\"span\", class_ = \"srp-skills\").text.strip()\n",
    "            skills = re.sub(r\"\\s*,\\s*\", ', ', skills)\n",
    "\n",
    "            more_info = job.header.h2.a['href']\n",
    "\n",
    "            if unfamiliar_skill not in skills:\n",
    "                with open(f'posts/{index}.txt', 'w') as f:\n",
    "                    f.write(f'{job_name}\\n')\n",
    "                    f.write(f'{company_name}\\n')\n",
    "                    f.write(f'{skills}\\n')\n",
    "                print(f'File Saved:{index}')\n",
    "                \n",
    "                csv_writter.writerow([job_name, company_name, skills, more_info])\n",
    "#                     print(f\"\"\"\n",
    "#                     Job Name: {job_name}\n",
    "#                     Company Name: {company_name}\n",
    "#                     skills Set: {skills}\n",
    "#                     \"Job Link\": {more_info}\n",
    "#                     \"\"\")   \n",
    "# if __name__ == '__main__':\n",
    "#     while True:\n",
    "#         find_jobs()\n",
    "#         time_wait = 1\n",
    "#         print(f\"Waiting {time_wait} minutes...\")\n",
    "#         time.sleep(time_wait * 60)\n",
    "\n",
    "find_jobs()  \n",
    "csv_file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ab120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7448bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://www.pcmag.com/news\")\n",
    "#r = r.text\n",
    "#print(r)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9eee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(r, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "579b94ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "test = soup.find(\"h3\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edeb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d889a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Extract the article title\n",
    "article_title_element = soup.select_one('h2 a')\n",
    "article_title = article_title_element['data-item'] if article_title_element else None\n",
    "print(article_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bae9a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the page. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "url = \"https://www.pcmag.com/news\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "#time.sleep(2)  # Wait for 2 seconds before making the request\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the news article titles\n",
    "    article_titles = soup.find_all('h2', class_='text-base md:text-xl font-bold')\n",
    "\n",
    "    # Extract and print the titles\n",
    "    for title in article_titles:\n",
    "        article_title = title.a.text.strip()\n",
    "        print(article_title)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "899fb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "title = soup.find_all('h2')\n",
    "title = [x.text.strip() for x in title]\n",
    "\n",
    "description = soup.find_all('p', {'class': 'line-clamp-2'})\n",
    "description = [x.text.strip() for x in description]\n",
    "\n",
    "date = soup.find_all('span', {'class': 'hidden md:inline-block mr-3'})\n",
    "date = [x.text.strip() for x in date]\n",
    "\n",
    "author = soup.find_all('a', {'data-element': 'author-name'})\n",
    "author = [x.text.strip() for x in author]\n",
    "\n",
    "# Iterate through the lists and print each item\n",
    "for i in range(len(description)):\n",
    "    print(f'Title: {title[i]}')\n",
    "    print(f'Description: {description[i]}')\n",
    "    print(f'Date: {date[i]}')\n",
    "    print(f'Author: {author[i]}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef53cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a4473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5cd38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7bdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376ac84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aef23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86483cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bff55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e94edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd67892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fbcafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296b199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553878e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a53b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf92426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d32d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c06668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180a495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39cbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ac6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29b4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384e603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdba9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc5049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183088d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9466adf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d9b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf578d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbd206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840316a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce3cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9a503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654e88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d9319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49591fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee65c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71190c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9219706c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96040bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dac0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa96e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ce1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74a443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc5369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23573587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
